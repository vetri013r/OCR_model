# -*- coding: utf-8 -*-
"""OCR Model.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1aCsYAcD7JdSdETo4gNO-_ql3_u9F_HNJ
"""

!pip install transformers

from transformers import TrOCRProcessor, VisionEncoderDecoderModel
from PIL import Image
import numpy as np
import onnxruntime as ort
import torch

# Load the TrOCR processor and model

# Load and preprocess the imag

import psutil
import time

# Function to get CPU utilization
def get_cpu_utilization():
    return psutil.cpu_percent(interval=1)

# Function to get memory usage
def get_memory_usage():
    memory_info = psutil.virtual_memory()
    return {
        'total': memory_info.total,
        'available': memory_info.available,
        'used': memory_info.used,
        'percent': memory_info.percent
    }

# Function to print memory usage
def print_memory_usage(memory_info):
    print(f"Memory Usage:")
    print(f"  Total: {memory_info['total'] / (1024 ** 3):.2f} GB")
    print(f"  Available: {memory_info['available'] / (1024 ** 3):.2f} GB")
    print(f"  Used: {memory_info['used'] / (1024 ** 3):.2f} GB")
    print(f"  Percent Used: {memory_info['percent']}%")

# Measure CPU and memory before code execution
print("System Metrics Before Code Execution:")
cpu_before = get_cpu_utilization()
memory_before = get_memory_usage()
print(f"CPU Utilization: {cpu_before}%")
print_memory_usage(memory_before)

# Your code block here
# Your code block here
processor = TrOCRProcessor.from_pretrained("microsoft/trocr-base-handwritten")
model = VisionEncoderDecoderModel.from_pretrained("microsoft/trocr-base-handwritten")
image = Image.open("/content/download.jpg").convert("RGB") # Replace with your image path

pixel_values = processor(image, return_tensors="pt").pixel_values

# Generate the text
generated_ids = model.generate(pixel_values, max_new_tokens=50)

# Decode the generated IDs
generated_text = processor.batch_decode(generated_ids, skip_special_tokens=True)[0]
# Example: Perform a computation
start_time = time.time()
result = sum(range(10000000))
end_time = time.time()

# Measure CPU and memory after code execution
cpu_after = get_cpu_utilization()
memory_after = get_memory_usage()
execution_time = end_time - start_time

print("\nSystem Metrics After Code Execution:")
print(f"CPU Utilization: {cpu_after}%")
print_memory_usage(memory_after)
print(f"Execution Time: {execution_time:.2f} seconds")

!nvidia-smi
!pip install GPUtil
!pip install --upgrade torch torchvision torchaudio
!pip install onnxruntime
!pip install torch transformers onnx onnxruntime onnxruntime-tools

import GPUtil
import time
import torch
import torchvision.models as models
# Function to get GPU memory usage
def get_gpu_memory():
    gpus = GPUtil.getGPUs()
    gpu_info = []
    for gpu in gpus:
        gpu_info.append({
            'id': gpu.id,
            'name': gpu.name,
            'memory_free': gpu.memoryFree,
            'memory_used': gpu.memoryUsed,
            'memory_total': gpu.memoryTotal
        })
    return gpu_info

# Function to print GPU memory usage
def print_gpu_memory_usage(info):
    for gpu in info:
        print(f"GPU {gpu['id']} ({gpu['name']}):")
        print(f"  Free Memory: {gpu['memory_free']} MB")
        print(f"  Used Memory: {gpu['memory_used']} MB")
        print(f"  Total Memory: {gpu['memory_total']} MB")

# Measure GPU memory before code execution
print("GPU Memory Usage Before Code Execution:")
gpu_info_before = get_gpu_memory()
print_gpu_memory_usage(gpu_info_before)
processor = TrOCRProcessor.from_pretrained("microsoft/trocr-base-handwritten")
model = VisionEncoderDecoderModel.from_pretrained("microsoft/trocr-base-handwritten")
image = Image.open("/content/image.jpg").convert("RGB") # Replace with your image path

pixel_values = processor(image, return_tensors="pt").pixel_values

# Generate the text
generated_ids = model.generate(pixel_values, max_new_tokens=50)

# Decode the generated IDs
generated_text = processor.batch_decode(generated_ids, skip_special_tokens=True)[0]
# Code block to perform computation using GPU
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
model = models.resnet18(pretrained=True).to(device)
inputs = torch.randn(1, 3, 224, 224).to(device)

# Measure time for inference
start_time = time.time()
outputs = model(inputs)
end_time = time.time()

print(f"\nTime taken for inference: {end_time - start_time:.2f} seconds")

# Measure GPU memory after code execution
print("\nGPU Memory Usage After Code Execution:")
gpu_info_after = get_gpu_memory()
print_gpu_memory_usage(gpu_info_after)

# Load the TrOCR processor and model
processor = TrOCRProcessor.from_pretrained("microsoft/trocr-base-handwritten")
model = VisionEncoderDecoderModel.from_pretrained("microsoft/trocr-base-handwritten")

# Define a dummy input for ONNX export
dummy_image = torch.randn(1, 3, 384, 384)  # Adjust dimensions if necessary

# Create dummy decoder input IDs - required for the decoder
dummy_decoder_input_ids = torch.tensor([[1, 2, 3, 4, 5]]) # Example - adjust length as needed

# Export the model to ONNX format
onnx_path = "trocr_model.onnx"
torch.onnx.export(
    model,
    (dummy_image, dummy_decoder_input_ids), # Pass both dummy inputs to the model
    onnx_path,
    input_names=['pixel_values', 'decoder_input_ids'], # Specify names for both inputs
    output_names=['logits'],
    opset_version=11
)

print(f"Model exported to {onnx_path}")

from onnxruntime.quantization import quantize_dynamic, QuantType

# Quantize the ONNX model
quantized_onnx_path = "/content/trocr_model.onnx"
quantize_dynamic(
    onnx_path,
    quantized_onnx_path,
    weight_type=QuantType.QUInt8
)

print(f"Quantized model saved to {quantized_onnx_path}")

# Load the TrOCR processor
processor = TrOCRProcessor.from_pretrained("microsoft/trocr-base-handwritten")

# Preprocess the image
def preprocess_image(image_path, processor):
    image = Image.open(image_path).convert("RGB")
    pixel_values = processor(image, return_tensors="pt").pixel_values
    return pixel_values.numpy()

# Perform inference
def perform_inference(model_path, pixel_values, processor):
    session = ort.InferenceSession(model_path, providers=['CPUExecutionProvider'])

    # Get input names
    input_name = session.get_inputs()[0].name  # This should be 'pixel_values'
    decoder_input_ids_name = session.get_inputs()[1].name  # This should be 'decoder_input_ids'

    # Create decoder_input_ids
    decoder_input_ids = torch.tensor([[processor.tokenizer.bos_token_id]], dtype=torch.long)

    # Run inference with both inputs
    outputs = session.run(None, {input_name: pixel_values, decoder_input_ids_name: decoder_input_ids.numpy()})
    return outputs

# Decode the outputs
def decode_outputs(outputs, processor):
    generated_ids = torch.tensor(outputs[0])
    generated_text = processor.batch_decode(generated_ids, skip_special_tokens=True)[0]
    return generated_text

# Paths
image_path = '/content/download.jpg'  # Replace with your image file path
onnx_model_path = '/content/trocr_model.onnx'  # Replace with your ONNX model path

# Workflow
pixel_values = preprocess_image(image_path, processor)
outputs = perform_inference(onnx_model_path, pixel_values, processor) # Pass the processor to the function
generated_text = decode_outputs(outputs, processor)

print("Generated Text:", generated_text)